<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="PixelCLIP">
  <meta property="og:title" content="PixelCLIP"/>
  <meta property="og:description" content="Towards Open-Vocabulary Semantic Segmentation Without Semantic Labels"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="PixelCLIP">
  <meta name="twitter:description" content="Towards Open-Vocabulary Semantic Segmentation Without Semantic Labels">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Towards Open-Vocabulary Semantic Segmentation Without Semantic Labels</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Towards Open-Vocabulary Semantic Segmentation Without Semantic Labels</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=zu-I2fYAAAAJ&hl=en&oi=ao" target="_blank">Heeseong Shin</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="https://kchyun.github.io/" target="_blank">Chaehyun Kim</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="https://sunghwanhong.github.io/" target="_blank">Sunghwan Hong</a><sup>2</sup>,</span>
              <span class="author-block">
                <a href="http://seokju-cho.github.io/" target="_blank">Seokju Cho</a><sup>1</sup>,</span>
                <br>
              <span class="author-block">
                <a href="https://anuragarnab.github.io" target="_blank">Anurag Arnab</a><sup>†,3</sup>,</span>
              <span class="author-block">
                <a href="https://phseo.github.io" target="_blank">Paul Hongsuck Seo</a><sup>†,2</sup>,</span>
              <span class="author-block">
                <a href="https://cvlab.kaist.ac.kr/members/faculty" target="_blank">Seungryong Kim</a><sup>†,1</sup>
              </span>                    
                <div>
                </div>


                </div>

                  <div class="is-size-5 publication-authors">
                      <span class="author-block"><sup>1</sup>KAIST, <sup>2</sup>Korea University, <sup>3</sup>Google Research</span>
                      <br>
                      <span class="author-block"><sup>†</sup>Co-Corresponding Author</span>
                      <!-- <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span> -->
                    </div>
                    <div>
                    <span class="is-size-5 publication-venue">NeurIPS 2024</span>

                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/2409.19846.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <!-- Supplementary PDF link -->
                    <!-- <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span> -->

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/cvlab-kaist/PixelCLIP" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2409.19846" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="static/images/fig1_final.jpg" alt="Teaser image" class="teaser-image">
      <h2 class="content has-text-justified">
        In contrast to existing methods utilizing (a) pixel-level semantic labels or (b) image-level semantic labels, we leverage unlabeled masks as supervision, which can be freely generated from vision foundation models such as SAM and DINO.
      </h2>
    </div>
  </div>
</section>
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Large-scale vision-language models like CLIP have demonstrated impressive open-vocabulary capabilities for image-level tasks, excelling in recognizing what objects are present. However, they struggle with pixel-level recognition tasks like semantic segmentation, which require understanding where the objects are located. In this work, we propose a novel method, PixelCLIP, to adapt the CLIP image encoder for pixel-level understanding by guiding the model on where, which is achieved using unlabeled images and masks generated from vision foundation models such as SAM and DINO. To address the challenges of leveraging masks without semantic labels, we devise an online clustering algorithm using learnable class names to acquire general semantic concepts. PixelCLIP shows significant performance improvements over CLIP and competitive results compared to caption-supervised methods in open-vocabulary semantic segmentation.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<section class="hero section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3 has-text-centered">Motivation</h2>
        <img src="static/images/clustering.png" alt="Motivation image" class="method">
        <h2 class="content has-text-justified">
          Recent vision foundation models (VFMs), such as DINO and SAM can freely generate fine-grained masks. The generated masks can be a guide to CLIP for pixel-level semantic understanding.
          However, the resulting masks can be too small or incomplete to have semantic meaning. To address this over-segmentation issue, we propose an online clustering of the masks into semantically meaningful groups defined globally for given images.
          By guiding CLIP with the clustered masks, we can adapt the CLIP image encoder to open-vocabulary semantic segmentation without any semantic labels.
        </h2>
      </div>
  </div>
</section>

<section class="hero section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3 has-text-centered">Architecture</h2>
        <h2 class="content has-text-justified">
          PixelCLIP utilizes unlabeled images and masks for fine-tuning the image encoder of CLIP, enabling open-vocabulary semantic segmentation. The momentum image encoder and the mask decoder are only leveraged during training, and inference is only done with image and text encoders of CLIP.
        </h2>
    <div class="hero-body">
      <img src="static/images/figure.jpg" alt="Method image" class="method">
    </div>
  </div>
</section>

<section class="hero section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3 has-text-centered">Qualitative Results</h2>
        <span class="content has-text-justified">
          Qualitative results of ADE20k with 150 categories.      
        </span>
        <img src="static/images/ade.jpg" alt="ADE Qual image" class="method">
        <span class="content has-text-justified">
          Qualitative results of Pascal-Context with 59 categories.      
        </span>
        <img src="static/images/pc59.jpg" alt="PC59 Qual image" class="method">
    </div>
  </div>
</section>

<section class="hero section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3 has-text-centered">Quantitative Results</h2>
        <span class="content has-text-justified">
          The best-performing results are presented in <strong>bold</strong>, while the second-best results are <u>underlined</u>. <br>
          *: Images were seen during training. †: Masks from SA-1B were used.
        </span>
      <img src="static/images/table.png" alt="Quan image" class="method">
  </div>
</section>



<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@misc{shin2024openvocabularysemanticsegmentationsemantic,
      title={Towards Open-Vocabulary Semantic Segmentation Without Semantic Labels}, 
      author={Heeseong Shin and Chaehyun Kim and Sunghwan Hong and Seokju Cho and Anurag Arnab and Paul Hongsuck Seo and Seungryong Kim},
      year={2024},
      eprint={2409.19846},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2409.19846}, 
}</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
